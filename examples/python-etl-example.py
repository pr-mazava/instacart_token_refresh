#!/usr/bin/env python3
"""
================================================================================
MAZAVA CONSULTING - INSTACART ADS API ETL PIPELINE
================================================================================

This ETL pipeline demonstrates how to use refresh tokens generated by the
instacart_refresh_token.py utility to build a production-grade data integration
between Instacart Ads API and various data destinations.

Author: Paul Rakotoarisoa <paul@mazavaltd.com>
Company: Mazava Consulting
Version: 1.0.0

Features:
- Multi-client support
- Automatic token refresh
- Error handling and retry logic
- Data transformation and validation
- Multiple destination support (CSV, Database, Cloud Storage)
- Logging and monitoring
- Configuration management

Usage:
    python python-etl-example.py --config config.json

#AnalyzeResponsibly
================================================================================
"""

import json
import logging
import pandas as pd
import requests
import time
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from pathlib import Path
import argparse
import schedule

# Optional dependencies for different destinations
try:
    from sqlalchemy import (
        create_engine,
        MetaData,
        Table,
        Column,
        String,
        Float,
        Integer,
        Date,
    )

    SQLALCHEMY_AVAILABLE = True
except ImportError:
    SQLALCHEMY_AVAILABLE = False
    print("SQLAlchemy not available - database features disabled")

try:
    import boto3

    AWS_AVAILABLE = True
except ImportError:
    AWS_AVAILABLE = False
    print("Boto3 not available - AWS features disabled")

try:
    from google.cloud import storage, bigquery

    GCP_AVAILABLE = True
except ImportError:
    GCP_AVAILABLE = False
    print("Google Cloud libraries not available - GCP features disabled")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("instacart_etl.log"), logging.StreamHandler()],
)
logger = logging.getLogger(__name__)


@dataclass
class InstacartClient:
    """Configuration for an Instacart API client"""

    name: str
    client_id: str
    client_secret: str
    refresh_token: str
    enabled: bool = True


@dataclass
class ETLConfig:
    """ETL Pipeline Configuration"""

    clients: List[InstacartClient]
    days_back: int = 30
    output_format: str = "csv"  # csv, database, s3, bigquery
    output_path: str = "./output"
    database_url: Optional[str] = None
    s3_bucket: Optional[str] = None
    bigquery_project: Optional[str] = None
    bigquery_dataset: Optional[str] = None
    retry_attempts: int = 3
    retry_delay: int = 2


class InstacartAPIClient:
    """Enhanced Instacart API client with refresh token management"""

    def __init__(self, client_config: InstacartClient):
        self.config = client_config
        self.access_token = None
        self.token_expires_at = None
        self.base_url = "https://api.ads.instacart.com/api/v2"
        self.auth_url = "https://api.ads.instacart.com/oauth/token"

        # Session for connection reuse
        self.session = requests.Session()
        self.session.headers.update(
            {"User-Agent": "Mazava-Instacart-ETL/1.0.0", "Accept": "application/json"}
        )

    def _refresh_access_token(self) -> str:
        """Refresh the access token using the refresh token"""
        logger.info(f"Refreshing access token for client: {self.config.name}")

        auth_payload = {
            "client_id": self.config.client_id,
            "client_secret": self.config.client_secret,
            "refresh_token": self.config.refresh_token,
            "grant_type": "refresh_token",
        }

        response = self.session.post(self.auth_url, json=auth_payload, timeout=30)
        response.raise_for_status()

        auth_data = response.json()
        self.access_token = auth_data["access_token"]

        # Calculate token expiration (tokens typically last 1 hour)
        expires_in = auth_data.get("expires_in", 3600)
        self.token_expires_at = datetime.now() + timedelta(
            seconds=expires_in - 300
        )  # 5 min buffer

        # Update session headers
        self.session.headers["Authorization"] = f"Bearer {self.access_token}"

        logger.info(f"Access token refreshed for {self.config.name}")
        return self.access_token

    def _ensure_valid_token(self):
        """Ensure we have a valid access token"""
        if (
            not self.access_token
            or not self.token_expires_at
            or datetime.now() >= self.token_expires_at
        ):
            self._refresh_access_token()

    def _make_api_request(self, endpoint: str, payload: dict, retries: int = 3) -> dict:
        """Make an authenticated API request with retry logic"""
        self._ensure_valid_token()

        url = f"{self.base_url}{endpoint}"

        for attempt in range(retries):
            try:
                response = self.session.post(url, json=payload, timeout=60)

                if response.status_code == 401:
                    # Token might be invalid, refresh and retry
                    logger.warning(
                        f"401 error, refreshing token for {self.config.name}"
                    )
                    self._refresh_access_token()
                    continue

                response.raise_for_status()
                return response.json()

            except requests.exceptions.RequestException as e:
                logger.warning(f"API request attempt {attempt + 1} failed: {e}")
                if attempt == retries - 1:
                    raise
                time.sleep(2**attempt)  # Exponential backoff

        raise Exception(f"Failed to make API request after {retries} attempts")

    def get_campaign_data(self, start_date: str, end_date: str) -> pd.DataFrame:
        """Fetch campaign performance data"""
        logger.info(
            f"Fetching campaign data for {self.config.name}: {start_date} to {end_date}"
        )

        payload = {
            "date_range": {"start_date": start_date, "end_date": end_date},
            "segment": "day",
            "sort_by": "date_asc",
            "filters": {"spend": True},
        }

        response_data = self._make_api_request("/reports/campaigns", payload)

        if not response_data.get("data"):
            logger.warning(f"No campaign data returned for {self.config.name}")
            return pd.DataFrame()

        # Convert to DataFrame
        columns = [
            "date",
            "campaign_id",
            "campaign_name",
            "status",
            "budget",
            "spend",
            "impressions",
            "clicks",
            "ctr",
            "cpc",
            "conversions",
            "sales",
            "roas",
        ]

        df = pd.DataFrame(
            response_data["data"], columns=columns[: len(response_data["data"][0])]
        )
        df["client_name"] = self.config.name
        df["data_type"] = "campaign"
        df["extracted_at"] = datetime.now()

        logger.info(f"Retrieved {len(df)} campaign records for {self.config.name}")
        return df

    def get_product_data(self, start_date: str, end_date: str) -> pd.DataFrame:
        """Fetch product performance data"""
        logger.info(
            f"Fetching product data for {self.config.name}: {start_date} to {end_date}"
        )

        payload = {
            "date_range": {"start_date": start_date, "end_date": end_date},
            "segment": "day",
            "sort_by": "date_asc",
            "filters": {"spend": True},
        }

        response_data = self._make_api_request("/reports/products", payload)

        if not response_data.get("data"):
            logger.warning(f"No product data returned for {self.config.name}")
            return pd.DataFrame()

        # Convert to DataFrame
        columns = [
            "date",
            "product_id",
            "product_name",
            "campaign_id",
            "campaign_name",
            "spend",
            "impressions",
            "clicks",
            "conversions",
            "sales",
            "units_sold",
        ]

        df = pd.DataFrame(
            response_data["data"], columns=columns[: len(response_data["data"][0])]
        )
        df["client_name"] = self.config.name
        df["data_type"] = "product"
        df["extracted_at"] = datetime.now()

        logger.info(f"Retrieved {len(df)} product records for {self.config.name}")
        return df

    def cleanup(self):
        """Clean up resources"""
        self.session.close()


class InstacartETLPipeline:
    """Main ETL Pipeline orchestrator"""

    def __init__(self, config: ETLConfig):
        self.config = config
        self.clients = {}

        # Initialize API clients
        for client_config in config.clients:
            if client_config.enabled:
                self.clients[client_config.name] = InstacartAPIClient(client_config)

    def _calculate_date_range(self) -> tuple:
        """Calculate start and end dates for data extraction"""
        end_date = datetime.now() - timedelta(days=1)  # Yesterday
        start_date = end_date - timedelta(days=self.config.days_back - 1)

        return start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")

    def _transform_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Apply data transformations and validations"""
        if df.empty:
            return df

        # Data type conversions
        numeric_columns = ["spend", "impressions", "clicks", "conversions", "sales"]
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0)

        # Date conversion
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"])

        # Add calculated metrics
        if "impressions" in df.columns and "clicks" in df.columns:
            df["ctr_calculated"] = df["clicks"] / df["impressions"].replace(0, 1)

        if "spend" in df.columns and "sales" in df.columns:
            df["roas_calculated"] = df["sales"] / df["spend"].replace(0, 1)

        if "clicks" in df.columns and "conversions" in df.columns:
            df["conversion_rate"] = df["conversions"] / df["clicks"].replace(0, 1)

        # Data quality checks
        initial_rows = len(df)

        # Remove rows with no spend or impressions
        df = df[(df.get("spend", 0) > 0) | (df.get("impressions", 0) > 0)]

        removed_rows = initial_rows - len(df)
        if removed_rows > 0:
            logger.info(f"Removed {removed_rows} rows with no activity")

        return df

    def extract_all_data(self) -> Dict[str, pd.DataFrame]:
        """Extract data from all configured clients"""
        start_date, end_date = self._calculate_date_range()
        logger.info(f"Extracting data for date range: {start_date} to {end_date}")

        all_data = {"campaigns": [], "products": []}

        for client_name, client in self.clients.items():
            try:
                # Extract campaign data
                campaign_df = client.get_campaign_data(start_date, end_date)
                if not campaign_df.empty:
                    all_data["campaigns"].append(campaign_df)

                # Extract product data
                product_df = client.get_product_data(start_date, end_date)
                if not product_df.empty:
                    all_data["products"].append(product_df)

                # Rate limiting
                time.sleep(1)

            except Exception as e:
                logger.error(f"Failed to extract data for {client_name}: {e}")
                continue

        # Combine data from all clients
        combined_data = {}
        for data_type, dfs in all_data.items():
            if dfs:
                combined_df = pd.concat(dfs, ignore_index=True)
                combined_data[data_type] = self._transform_data(combined_df)
                logger.info(f"Combined {data_type}: {len(combined_df)} total records")
            else:
                combined_data[data_type] = pd.DataFrame()
                logger.warning(f"No {data_type} data extracted")

        return combined_data

    def save_to_csv(self, data: Dict[str, pd.DataFrame]):
        """Save data to CSV files"""
        output_dir = Path(self.config.output_path)
        output_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        for data_type, df in data.items():
            if not df.empty:
                filename = f"instacart_{data_type}_{timestamp}.csv"
                filepath = output_dir / filename
                df.to_csv(filepath, index=False)
                logger.info(f"Saved {len(df)} {data_type} records to {filepath}")

    def save_to_database(self, data: Dict[str, pd.DataFrame]):
        """Save data to SQL database"""
        if not SQLALCHEMY_AVAILABLE:
            raise Exception(
                "SQLAlchemy not available - install with: pip install sqlalchemy"
            )

        if not self.config.database_url:
            raise Exception("Database URL not configured")

        engine = create_engine(self.config.database_url)

        for data_type, df in data.items():
            if not df.empty:
                table_name = f"instacart_{data_type}"
                df.to_sql(table_name, engine, if_exists="append", index=False)
                logger.info(
                    f"Saved {len(df)} {data_type} records to database table {table_name}"
                )

    def save_to_s3(self, data: Dict[str, pd.DataFrame]):
        """Save data to AWS S3"""
        if not AWS_AVAILABLE:
            raise Exception("Boto3 not available - install with: pip install boto3")

        if not self.config.s3_bucket:
            raise Exception("S3 bucket not configured")

        s3_client = boto3.client("s3")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        for data_type, df in data.items():
            if not df.empty:
                # Convert to CSV string
                csv_string = df.to_csv(index=False)

                # Upload to S3
                key = f"instacart/{data_type}/year={datetime.now().year}/month={datetime.now().month:02d}/instacart_{data_type}_{timestamp}.csv"

                s3_client.put_object(
                    Bucket=self.config.s3_bucket,
                    Key=key,
                    Body=csv_string,
                    ContentType="text/csv",
                )

                logger.info(
                    f"Saved {len(df)} {data_type} records to S3: s3://{self.config.s3_bucket}/{key}"
                )

    def save_to_bigquery(self, data: Dict[str, pd.DataFrame]):
        """Save data to Google BigQuery"""
        if not GCP_AVAILABLE:
            raise Exception(
                "Google Cloud libraries not available - install with: pip install google-cloud-bigquery"
            )

        if not self.config.bigquery_project or not self.config.bigquery_dataset:
            raise Exception("BigQuery project/dataset not configured")

        client = bigquery.Client(project=self.config.bigquery_project)

        for data_type, df in data.items():
            if not df.empty:
                table_id = f"{self.config.bigquery_project}.{self.config.bigquery_dataset}.instacart_{data_type}"

                job_config = bigquery.LoadJobConfig(
                    write_disposition="WRITE_APPEND", autodetect=True
                )

                job = client.load_table_from_dataframe(
                    df, table_id, job_config=job_config
                )
                job.result()  # Wait for job completion

                logger.info(
                    f"Saved {len(df)} {data_type} records to BigQuery table {table_id}"
                )

    def run_pipeline(self):
        """Execute the complete ETL pipeline"""
        start_time = datetime.now()
        logger.info("Starting Instacart ETL pipeline")

        try:
            # Extract
            data = self.extract_all_data()

            # Load to configured destination
            if self.config.output_format == "csv":
                self.save_to_csv(data)
            elif self.config.output_format == "database":
                self.save_to_database(data)
            elif self.config.output_format == "s3":
                self.save_to_s3(data)
            elif self.config.output_format == "bigquery":
                self.save_to_bigquery(data)
            else:
                raise Exception(
                    f"Unsupported output format: {self.config.output_format}"
                )

            # Generate summary
            total_records = sum(len(df) for df in data.values())
            duration = datetime.now() - start_time

            logger.info(f"ETL pipeline completed successfully")
            logger.info(f"Total records processed: {total_records}")
            logger.info(f"Duration: {duration.total_seconds():.2f} seconds")

        except Exception as e:
            logger.error(f"ETL pipeline failed: {e}")
            raise

        finally:
            # Cleanup
            for client in self.clients.values():
                client.cleanup()


def load_config(config_path: str) -> ETLConfig:
    """Load configuration from JSON file"""
    with open(config_path, "r") as f:
        config_data = json.load(f)

    clients = []
    for client_data in config_data.get("clients", []):
        clients.append(InstacartClient(**client_data))

    return ETLConfig(
        clients=clients,
        days_back=config_data.get("days_back", 30),
        output_format=config_data.get("output_format", "csv"),
        output_path=config_data.get("output_path", "./output"),
        database_url=config_data.get("database_url"),
        s3_bucket=config_data.get("s3_bucket"),
        bigquery_project=config_data.get("bigquery_project"),
        bigquery_dataset=config_data.get("bigquery_dataset"),
        retry_attempts=config_data.get("retry_attempts", 3),
        retry_delay=config_data.get("retry_delay", 2),
    )


def create_sample_config():
    """Create a sample configuration file"""
    sample_config = {
        "clients": [
            {
                "name": "client1",
                "client_id": "your_client1_id_here",
                "client_secret": "your_client1_secret_here",
                "refresh_token": "your_client1_refresh_token_here",
                "enabled": True,
            },
            {
                "name": "client2",
                "client_id": "your_client2_id_here",
                "client_secret": "your_client2_secret_here",
                "refresh_token": "your_client2_refresh_token_here",
                "enabled": True,
            },
        ],
        "days_back": 30,
        "output_format": "csv",
        "output_path": "./output",
        "database_url": "something",
        "s3_bucket": "something",
        "bigquery_project": "something",
        "bigquery_dataset": "something",
        "retry_attempts": 3,
        "retry_delay": 2,
    }

    with open("config.json", "w") as f:
        json.dump(sample_config, f, indent=2)

    print("Sample configuration saved to config.json")
    print("Update the client credentials and run again!")


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description="Instacart Ads API ETL Pipeline")
    parser.add_argument(
        "--config", default="config.json", help="Path to configuration file"
    )
    parser.add_argument(
        "--create-config", action="store_true", help="Create sample configuration file"
    )
    parser.add_argument(
        "--schedule", action="store_true", help="Run in scheduled mode (daily at 6 AM)"
    )

    args = parser.parse_args()

    if args.create_config:
        create_sample_config()
        return

    if not os.path.exists(args.config):
        print(f"Configuration file {args.config} not found!")
        print("Run with --create-config to generate a sample configuration.")
        return

    try:
        config = load_config(args.config)
        pipeline = InstacartETLPipeline(config)

        if args.schedule:
            # Schedule daily runs
            schedule.every().day.at("06:00").do(pipeline.run_pipeline)
            logger.info("Scheduled daily ETL runs at 6:00 AM")

            while True:
                schedule.run_pending()
                time.sleep(60)
        else:
            # Run once
            pipeline.run_pipeline()

    except Exception as e:
        logger.error(f"Pipeline execution failed: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())


"""
================================================================================
EXAMPLE CONFIGURATION FILE (config.json)
================================================================================

{
  "clients": [
    {
      "name": "brand_a",
      "client_id": "your_brand_a_client_id",
      "client_secret": "your_brand_a_client_secret",
      "refresh_token": "your_brand_a_refresh_token_from_utility",
      "enabled": true
    },
    {
      "name": "brand_b", 
      "client_id": "your_brand_b_client_id",
      "client_secret": "your_brand_b_client_secret",
      "refresh_token": "your_brand_b_refresh_token_from_utility",
      "enabled": true
    }
  ],
  "days_back": 30,
  "output_format": "csv",
  "output_path": "./output",
  "database_url": "postgresql://user:pass@localhost:5432/instacart_data",
  "s3_bucket": "your-data-bucket",
  "bigquery_project": "your-gcp-project",
  "bigquery_dataset": "instacart_analytics",
  "retry_attempts": 3,
  "retry_delay": 2
}

================================================================================
USAGE EXAMPLES
================================================================================

# Create sample configuration
python python-etl-example.py --create-config

# Run ETL pipeline once  
python python-etl-example.py --config config.json

# Run in scheduled mode (daily at 6 AM)
python python-etl-example.py --config config.json --schedule

# Save to different destinations
# CSV (default)
"output_format": "csv"

# PostgreSQL/MySQL/SQL Server
"output_format": "database"
"database_url": "postgresql://user:pass@localhost:5432/db"

# AWS S3
"output_format": "s3" 
"s3_bucket": "your-bucket-name"

# Google BigQuery
"output_format": "bigquery"
"bigquery_project": "your-project"
"bigquery_dataset": "your_dataset"

================================================================================
DEPENDENCIES
================================================================================

# Required
pip install pandas requests

# Optional (install as needed)
pip install sqlalchemy psycopg2-binary  # Database support
pip install boto3                       # AWS S3 support  
pip install google-cloud-bigquery      # BigQuery support
pip install schedule                    # Scheduling support

================================================================================
For support: paul@mazavaltd.com | #AnalyzeResponsibly
================================================================================
"""
